# Setting up Python environments on LUMI: Or why you shouldn't use the LUMI container wrapper

This guide provides instructions on how to create custom Python environments where inter-node communication works properly. This is essential for running multi-node training jobs on LUMI using e.g. PyTorch, TensorFlow, and JAX.

All the information provided here is also available in LUMI official documentation, but as the information is scattered across multiple pages, it can be difficult to find the relevant details. This guide aims to consolidate that information and provide a clear, step-by-step process for setting up your custom environment.

## Table of Contents

1.  [Motivation](#motivation)
2.  [Outline](#outline)
3.  [Building a Python environment with Cotainr](#building-a-python-environment-with-cotainr)
4.  [Running jobs with the environment](#running-jobs-with-the-environment)


## Motivation

Many users move to LUMI from CSC clusters like Mahti and Puhti, where it is convenient to create custom Python environments using the [Tykky tool](https://docs.csc.fi/computing/containers/tykky/) for wrapping Conda environments in Singularity containers. 

A similar tool, [LUMI container wrapper](https://docs.lumi-supercomputer.eu/software/installing/container-wrapper/), is also available on LUMI and is tempting to use because of the familiar workflow. However, using the container wrapper on LUMI has significant drawbacks, especially for multi-node training jobs. Let's consider an example of training a PyTorch model using Distributed Data Parallel (DDP) across two nodes. Here are the epoch durations with environments created in two different ways:

```bash
# With an environment created with LUMI container wrapper
Epoch 1: 100%|██████████| 2442/2442 [06:20<00:00,  6.42it/s]

# With an environment created following this guide
Epoch 0: 100%|██████████| 2442/2442 [00:49<00:00, 49.25it/s]
```
As you can see, the run with the LUMI container wrapper environment is ~7x slower! But why?

The reason is that fast node-to-node communication on LUMI requires the AWS OFI plugin for RCCL. RCCL is the AMD GPU collective communication library, replacement for NVIDIA NCCL. The plugin enables RCCL to use LUMI's Slingshot-11 interconnect, as RCCL does not support it out of the box. LUMI container wrapper installs are start from a minimal base image that does not include the AWS OFI plugin. While it is possible to manually inject the plugin, it is easier to build Python environments on top of pre-configured ROCm images that already include the plugin. This can be achieved using the [Cotainr tool](https://docs.lumi-supercomputer.eu/software/containers/singularity/#building-containers-using-the-cotainr-tool) available on LUMI.


## Building a Python environment with Cotainr

The following example code is available in this [GitHub repository](https://github.com/lasuomela/LUMI-Project-Template) so that it can be easily tested on LUMI.

To build the example environment, clone the repository and run the `create_environment.sh` script on LUMI:

```bash
cd /scratch/<your_LUMI_username>
git clone https://github.com/lasuomela/LUMI-Project-Template.git
cd LUMI-Project-Template

sbatch -A <your_LUMI_project_id> slurm_tools/create_environment.sh
```

The environment build is done on a compute node to ensure sufficient RAM and CPU. The script creates a Singularity image file that contains your Conda environment, and a SquashFS file that contains a venv with any Python packages that can't be installed directly with Conda. The build process takes some time, you can monitor the progress in the `log_build.out` and `log_build.err` files generated by SLURM.

## Running jobs with the environment

Once the environment is built, you can run an example PyTorch job like
```bash
sbatch -A <your_LUMI_project_id> slurm_tools/run.sh
```

You can change the number of GPUs and nodes in the `run.sh` script. If everything is set up correctly, you should see fast epoch times similar to the ones shown in the Motivation section, and the epoch times should decrease ~linearly as you increase the number of nodes used for training.